{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from html.parser import HTMLParser\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we will be using tensorflow to implement a BERT model. On top of the BERT layer, there will be an additional 128 neuron layer before the output. The output layer will consist of 2 neurons, giving the probabiliy of the candidate being correct or wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do get our data in the right input formart for BERT. BERT expects input data in a specific format, with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). Furthermore, we need to tokenize our text into tokens that correspond to BERT’s vocabulary.\n",
    "\n",
    "The BERT layer requires 3 input sequence: <br><br>\n",
    "<b>Token ids:</b> for every token in the sentence. This will be from the BERT vocab file <br><br>\n",
    "<b>Mask ids:</b> for every token to mask out tokens used only for the sequence padding (so every sequence has the same length). The max sequence length we are using will be 256, so if an example is only 200 tokens long, the frist 200 entires in Mask Ids will be 1 and last 56 entries will be 0 <br><br>\n",
    "<b>Segment ids:</b> 0 for one-sentence sequence, 1 if there are two sentences in the sequence and it is the second one<br><br>\n",
    "The data will have to be in the form of tensorflow tensors. We will now write the function to create input data with all these conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"assets/vocab.txt\" #Bert Vocab file\n",
    "df = pd.read_csv(\"assets/5000_rows.csv\")\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>questions</th>\n",
       "      <th>candidate</th>\n",
       "      <th>target</th>\n",
       "      <th>short_answer</th>\n",
       "      <th>has_sa</th>\n",
       "      <th>cos_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>667957236772653012</td>\n",
       "      <td>what was the number one song in june 1994</td>\n",
       "      <td>&lt;Table&gt; &lt;Tr&gt; &lt;Th&gt; Issue date &lt;/Th&gt; &lt;Th&gt; Song &lt;...</td>\n",
       "      <td>1</td>\n",
       "      <td>`` I Swear ''</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6844915846944003488</td>\n",
       "      <td>when did the live action scooby doo come out</td>\n",
       "      <td>&lt;P&gt; On Rotten Tomatoes , the film has an appro...</td>\n",
       "      <td>0</td>\n",
       "      <td>June 14 , 2002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6722765189438257616</td>\n",
       "      <td>who wrote what the hell did i say</td>\n",
       "      <td>&lt;P&gt; `` What the Hell Did I Say '' is a song by...</td>\n",
       "      <td>1</td>\n",
       "      <td>country music artist Dierks Bentley</td>\n",
       "      <td>1</td>\n",
       "      <td>0.348899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8133224726310396603</td>\n",
       "      <td>who narrated the original 1966 how the grinch ...</td>\n",
       "      <td>&lt;P&gt; Because Thurl Ravenscroft was not credited...</td>\n",
       "      <td>0</td>\n",
       "      <td>Boris Karloff</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7521828414792809112</td>\n",
       "      <td>when is it legal to smoke weed in canada</td>\n",
       "      <td>&lt;P&gt; In the 1960s cannabis began to rapidly inc...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.287019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ids                                          questions  \\\n",
       "0   667957236772653012          what was the number one song in june 1994   \n",
       "1  6844915846944003488       when did the live action scooby doo come out   \n",
       "2  6722765189438257616                  who wrote what the hell did i say   \n",
       "3  8133224726310396603  who narrated the original 1966 how the grinch ...   \n",
       "4  7521828414792809112           when is it legal to smoke weed in canada   \n",
       "\n",
       "                                           candidate  target  \\\n",
       "0  <Table> <Tr> <Th> Issue date </Th> <Th> Song <...       1   \n",
       "1  <P> On Rotten Tomatoes , the film has an appro...       0   \n",
       "2  <P> `` What the Hell Did I Say '' is a song by...       1   \n",
       "3  <P> Because Thurl Ravenscroft was not credited...       0   \n",
       "4  <P> In the 1960s cannabis began to rapidly inc...       0   \n",
       "\n",
       "                          short_answer  has_sa     cos_s  \n",
       "0                        `` I Swear ''       1  0.011881  \n",
       "1                       June 14 , 2002       1  0.229416  \n",
       "2  country music artist Dierks Bentley       1  0.348899  \n",
       "3                        Boris Karloff       1  0.394178  \n",
       "4                                  NaN       0  0.287019  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file=vocab_path)\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def tester(x,df,y=256,p=False,train=True):\n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    input_segments = []\n",
    "    \n",
    "    ml = y\n",
    "    for i in df.head(x).index:\n",
    "        ans = strip_tags(str(df.loc[i,'candidate']))\n",
    "        ans = ans.replace(\".\",\"[SEP]\")\n",
    "        ans = ans.replace('\"\"', '')\n",
    "        ans = ans.replace('``','')\n",
    "        ans = ans.replace(\"''\", \"\")\n",
    "        #ans = \"[CLS] \" + ans\n",
    "        ans = \" \".join(ans.split())\n",
    "\n",
    "        qn = strip_tags(str(df.loc[i,'questions']))\n",
    "        qn = qn.replace(\".\",\"[SEP]\")\n",
    "        qn = \"[CLS] \" + qn + \"[CLS]\"\n",
    "        fn = qn + \" \" + ans\n",
    "        if p:\n",
    "            print(fn)\n",
    "        toks = tokenizer.tokenize(fn)\n",
    "        tokens = toks[:ml]\n",
    "        input_ids.append(get_ids(tokens,tokenizer,ml))\n",
    "        input_masks.append(get_masks(tokens,ml))\n",
    "        input_segments.append(get_segments(tokens,ml))\n",
    "    \n",
    "    if train:\n",
    "        inputs = [tf.convert_to_tensor(input_ids,dtype=float),tf.convert_to_tensor(input_masks,dtype=float),tf.convert_to_tensor(input_segments,dtype=float)\n",
    "                ]\n",
    "\n",
    "        #output = tf.convert_to_tensor(df.head(x)['target'],dtype=tf.int32)\n",
    "        #return inputs,output\n",
    "\n",
    "        output = tf.one_hot(tf.convert_to_tensor(df.head(x)['target'],dtype=tf.int32),depth=2)\n",
    "        return inputs,output\n",
    "    else:\n",
    "        inputs = [tf.convert_to_tensor(input_ids,dtype=float),tf.convert_to_tensor(input_masks,dtype=float),tf.convert_to_tensor(input_segments,dtype=float)\n",
    "                ]\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what was the number one song in june 1994[CLS] Issue date Song Artist ( s ) Reference January 1 Hero Mariah Carey January 8 January 15 January 22 All for Love Bryan Adams / Rod Stewart / Sting January 29 February 5 February 12 The Power of Love Céline Dion February 19 February 26 March 5 March 12 The Sign Ace of Base March 19 March 26 April 2 April 9 Bump n ' Grind R[SEP] Kelly April 16 April 23 April 30 May 7 The Sign Ace of Base May 14 May 21 I Swear All - 4 - One May 28 June 4 June 11 June 18 June 25 July 2 July 9 July 16 July 23 July 30 August 6 Stay ( I Missed You ) Lisa Loeb & Nine Stories August 13 August 20 August 27 I 'll Make Love to You Boyz II Men September 3 September 10 September 17 September 24 October 1 October 8 October 15 October 22 October 29 November 5 November 12 November 19 November 26 December 3 On Bended Knee December 10 December 17 Here Comes the Hotstepper Ini Kamoze December 24 December 31 On Bended Knee Boyz II Men\n"
     ]
    }
   ],
   "source": [
    "inp,target = tester(1,df,p=True) #Example of the text being converted to input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is an example of the text before being conerted to the input formats. It beginds with the [CLS] token, followed by the question. After the question comes another [CLS] token and then the candidate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[  101.,  2054.,  2001.,  1996.,  2193.,  2028.,  2299.,  1999.,\n",
       "          2238.,  2807.,   101.,  3277.,  3058.,  2299.,  3063.,  1006.,\n",
       "          1055.,  1007.,  4431.,  2254.,  1015.,  5394.,  3814.,  2232.,\n",
       "         11782.,  2254.,  1022.,  2254.,  2321.,  2254.,  2570.,  2035.,\n",
       "          2005.,  2293.,  8527.,  5922.,  1013.,  8473.,  5954.,  1013.,\n",
       "         12072.,  2254.,  2756.,  2337.,  1019.,  2337.,  2260.,  1996.,\n",
       "          2373.,  1997.,  2293., 24550., 19542.,  2337.,  2539.,  2337.,\n",
       "          2656.,  2233.,  1019.,  2233.,  2260.,  1996.,  3696.,  9078.,\n",
       "          1997.,  2918.,  2233.,  2539.,  2233.,  2656.,  2258.,  1016.,\n",
       "          2258.,  1023., 16906.,  1050.,  1005., 23088.,  1054.,   102.,\n",
       "          5163.,  2258.,  2385.,  2258.,  2603.,  2258.,  2382.,  2089.,\n",
       "          1021.,  1996.,  3696.,  9078.,  1997.,  2918.,  2089.,  2403.,\n",
       "          2089.,  2538.,  1045.,  8415.,  2035.,  1011.,  1018.,  1011.,\n",
       "          2028.,  2089.,  2654.,  2238.,  1018.,  2238.,  2340.,  2238.,\n",
       "          2324.,  2238.,  2423.,  2251.,  1016.,  2251.,  1023.,  2251.,\n",
       "          2385.,  2251.,  2603.,  2251.,  2382.,  2257.,  1020.,  2994.,\n",
       "          1006.,  1045.,  4771.,  2017.,  1007.,  7059.,  8840., 15878.,\n",
       "          1004.,  3157.,  3441.,  2257.,  2410.,  2257.,  2322.,  2257.,\n",
       "          2676.,  1045.,  1005.,  2222.,  2191.,  2293.,  2000.,  2017.,\n",
       "          2879.,  2480.,  2462.,  2273.,  2244.,  1017.,  2244.,  2184.,\n",
       "          2244.,  2459.,  2244.,  2484.,  2255.,  1015.,  2255.,  1022.,\n",
       "          2255.,  2321.,  2255.,  2570.,  2255.,  2756.,  2281.,  1019.,\n",
       "          2281.,  2260.,  2281.,  2539.,  2281.,  2656.,  2285.,  1017.,\n",
       "          2006.,  8815.,  2098.,  6181.,  2285.,  2184.,  2285.,  2459.,\n",
       "          2182.,  3310.,  1996.,  2980., 13473., 18620.,  1999.,  2072.,\n",
       "         27829., 18153.,  2063.,  2285.,  2484.,  2285.,  2861.,  2006.,\n",
       "          8815.,  2098.,  6181.,  2879.,  2480.,  2462.,  2273.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp #The final input for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is a one hot encoded tensor. If the candidate is correct, it will be [0,1], otherwise it will be [1,0]. This format will allow us to use the Binary Crossentropy from logits as the loss function for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's create the model. The BERT layer has been donwloaded and saved in the assets folder. The BERT layer was downloaded from here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True,name=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_model():\n",
    "    max_seq_length = 256  # Your choice here.\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                           name=\"input_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                        name=\"segment_ids\")\n",
    "    pooled_output, sequence_output = bert_layer([input_ids, input_mask, segment_ids])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128,activation=tf.nn.relu,name='combined_layer',kernel_initializer=tf.random_normal_initializer)(pooled_output)\n",
    "    logits = tf.keras.layers.Dense(2,activation='softmax')(x)\n",
    "    model = Model(inputs=[input_ids,input_mask,segment_ids], outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mk_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has 109,580,930 trainable params, the output shape of the bert layer is (None,768) this feeds into a 128 neuron layer before the 2 neuron output logits layer. The model will be compiled using an Stochastic Gradient Descent optimizer and the loss will be Binary Cross Entropy. <br><br>\n",
    "\n",
    "Binary crossentropy measures how far away from the true value (which is either 0 or 1) the prediction is for each of the classes and then averages these class-wise errors to obtain the final loss. <br><br>\n",
    "\n",
    "Metrics used were the plain accuracy metric and the F1 score.\n",
    "\n",
    "The F1 score considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. (From Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "f1 = tfa.metrics.F1Score(num_classes=2)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01,momentum=0.1)\n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss = loss,\n",
    "              metrics=['accuracy',f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was training on a virtual instance with 1 GPU on Google Cloud Platform. Due to the large number of trainable params, the training could be done with a maximum batch size of 16 with only 1 GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Answer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a differnt function to extract data for the short answer model. We will no longer be needing long answer candidate rows. We will need each question on 1 row with it's correct long and short answer (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(t):\n",
    "    \n",
    "    ex_id = t['example_id']\n",
    "    q = t['question_text']\n",
    "    yn = t['annotations'][0]['yes_no_answer']\n",
    "    ans1 = t['annotations'][0]['long_answer']['start_token']\n",
    "    ans2 = t['annotations'][0]['long_answer']['end_token']\n",
    "    sp_doc = t['document_text'].split(\" \")\n",
    "    answer = \" \".join(sp_doc[ans1:ans2])\n",
    "    \n",
    "    if int(bool(t['annotations'][0]['short_answers'])):\n",
    "        sa = sp_doc[t['annotations'][0]['short_answers'][0]['start_token']:t['annotations'][0]['short_answers'][0]['end_token']]\n",
    "        sa = \" \".join(sa)\n",
    "        start = t['annotations'][0]['short_answers'][0]['start_token']\n",
    "        end = t['annotations'][0]['short_answers'][0]['end_token']\n",
    "    else:\n",
    "        sa = None\n",
    "        start = None\n",
    "        end = None\n",
    "    r = {\"ids\":[ex_id],\n",
    "         \"question\":[q],\n",
    "         \"answer\":[answer],\n",
    "         \"short_answer\": [sa],\n",
    "         \"has_yn\":[yn],\n",
    "         'start': [start],\n",
    "         'end':[end]\n",
    "         \n",
    "        }\n",
    "    r = pd.DataFrame(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_json_short(path,x):\n",
    "    with open(path, 'rt') as json_file:\n",
    "        ids = []\n",
    "        question = []\n",
    "        answer = []\n",
    "        sa = []\n",
    "        has_yn = []\n",
    "        start = []\n",
    "        end =[]\n",
    "        cnt = 0\n",
    "\n",
    "        \n",
    "        for line in json_file:\n",
    "            cnt += 1\n",
    "            t = json.loads(line) \n",
    "            results = get_data(t)\n",
    "                \n",
    "            ids.extend(results['ids'])\n",
    "            question.extend(results['question'])\n",
    "            answer.extend(results['answer'])\n",
    "            sa.extend(results['short_answer'])\n",
    "            has_yn.extend(results['has_yn'])\n",
    "            start.extend(results['start'])\n",
    "            end.extend(results['end'])\n",
    "\n",
    "            if cnt == x:\n",
    "                df = pd.DataFrame()\n",
    "                df['ids'] = ids\n",
    "                df['questions'] = question\n",
    "                df['answer'] = answer\n",
    "                df['short_answer'] = sa\n",
    "                df['has_yn'] = has_yn\n",
    "                df['start'] = start\n",
    "                df['end'] = end\n",
    "                break\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ext_json_short(\"assets/simplified-nq-train.jsonl\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yne(row): # Function to merge \"has_yn\" column into \"short_answer column\". \n",
    "    if row['has_yn'] != \"NONE\":\n",
    "        if row['has_yn'] == \"NO\":\n",
    "            val = \"No\"\n",
    "        elif row['has_yn'] == \"YES\":\n",
    "            val = \"Yes\"\n",
    "    else:\n",
    "        val = row['short_answer']\n",
    "    return val\n",
    "\n",
    "df['short_answer'] = df.apply(yne,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create short answer candidates from the correct long answer. This will be done by breaking the long answer up into sentences and also by isolating named entities and dates. Each question will also have a Yes & No as candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "cols = ['id',\"question\",'candidate','long_answer','target']\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "def created_sa_cands(df,x=False,y=False):\n",
    "    ndf = pd.DataFrame(columns=cols)\n",
    "    qids = []\n",
    "    qs = []\n",
    "    las =[]\n",
    "    cands = []\n",
    "    targets = []\n",
    "    if x:\n",
    "        df = df.loc[x:y]\n",
    "    for i in tqdm(df.index):\n",
    "        temp = []\n",
    "        df.loc[i,'answer'] = strip_tags(str(df.loc[i,'answer']))\n",
    "        \n",
    "        qid = df.loc[i,'ids']\n",
    "        q = df.loc[i,'questions']\n",
    "        la = df.loc[i,'answer']\n",
    "        sa = df.loc[i,'short_answer']\n",
    "        target = 1\n",
    "        qids.append(qid)\n",
    "        qs.append(q)\n",
    "        las.append(la)\n",
    "        cands.append(sa)\n",
    "        temp.append(sa)\n",
    "        targets.append(target)\n",
    "        sents = split_into_sentences(la)\n",
    "        for sent in sents:\n",
    "            qids.append(qid)\n",
    "            qs.append(q)\n",
    "            las.append(la)\n",
    "            cands.append(sent)\n",
    "            targets.append(0)\n",
    "        nes = preprocess(la)\n",
    "        text = \"\"\n",
    "        \n",
    "        for ne in nes:\n",
    "            \n",
    "            if ne[1] in [\"NNP\",\"CD\"]:\n",
    "                text += ne[0]\n",
    "                text += \" \"\n",
    "            elif ne[1] == \"CC\" and last_e in [\"NNP\",\"CD\"]:\n",
    "                text += ne[0]\n",
    "                text += \" \"\n",
    "            else:\n",
    "                if text and text not in temp:\n",
    "                    qids.append(qid)\n",
    "                    qs.append(q)\n",
    "                    las.append(la)\n",
    "                    cands.append(text)\n",
    "                    temp.append(text)\n",
    "                    targets.append(0)\n",
    "                    text = \"\"\n",
    "            last_e = ne[1]\n",
    "        qids.append(qid)\n",
    "        qs.append(q)\n",
    "        las.append(la)\n",
    "        cands.append(\"Yes\")\n",
    "        targets.append(0)\n",
    "        qids.append(qid)\n",
    "        qs.append(q)\n",
    "        las.append(la)\n",
    "        cands.append(\"No\")\n",
    "        targets.append(0)\n",
    "        \n",
    "    ndf['id'] = qids\n",
    "    ndf['question'] = qs\n",
    "    ndf['long_answer'] = las\n",
    "    ndf['candidate'] = cands\n",
    "    ndf['target'] = targets\n",
    "    return ndf\n",
    "\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 57.51it/s]\n"
     ]
    }
   ],
   "source": [
    "sa_df = created_sa_cands(df,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long answer:  Tracy McConnell , better known as `` The Mother '' , is the title character from the CBS television sitcom How I Met Your Mother . The show , narrated by Future Ted , tells the story of how Ted Mosby met The Mother . Tracy McConnell appears in 8 episodes from `` Lucky Penny '' to `` The Time Travelers '' as an unseen character ; she was first seen fully in `` Something New '' and was promoted to a main character in season 9 . The Mother is played by Cristin Milioti . \n",
      "\n",
      "Candidate 0 : Tracy McConnell\n",
      "Candidate 1 : Tracy McConnell , better known as `` The Mother '' , is the title character from the CBS television sitcom How I Met Your Mother .\n",
      "Candidate 2 : The show , narrated by Future Ted , tells the story of how Ted Mosby met The Mother .\n",
      "Candidate 3 : Tracy McConnell appears in 8 episodes from `` Lucky Penny '' to `` The Time Travelers '' as an unseen character ; she was first seen fully in `` Something New '' and was promoted to a main character in season 9 .\n",
      "Candidate 4 : The Mother is played by Cristin Milioti .\n",
      "Candidate 5 : Tracy McConnell \n",
      "Candidate 6 : Mother \n",
      "Candidate 7 : CBS \n",
      "Candidate 8 : How \n",
      "Candidate 9 : Mother Future Ted \n",
      "Candidate 10 : Ted Mosby \n",
      "Candidate 11 : Mother Tracy McConnell \n",
      "Candidate 12 : 8 \n",
      "Candidate 13 : Lucky Penny \n",
      "Candidate 14 : Time \n",
      "Candidate 15 : Something New \n",
      "Candidate 16 : 9 \n",
      "Candidate 17 : Mother Cristin Milioti \n",
      "Candidate 18 : Yes\n",
      "Candidate 19 : No\n"
     ]
    }
   ],
   "source": [
    "temp = sa_df.loc[sa_df['id'] == 5328212470870865242]\n",
    "temp.reset_index(inplace=True,drop=True)\n",
    "print(\"Long answer: {}\\n\".format(temp.loc[0,'long_answer'].replace(\"  \", \" \")))\n",
    "for i in temp.index:\n",
    "    print(\"Candidate {} : {}\".format(i,temp.loc[i,'candidate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need functions to prep the data to be inputed to the model. Simillar to the long answer functions but with slight modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def tester(x,df,y=256):\n",
    "    #del df\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    input_segments = []\n",
    "    p = list(df.head(x).index)\n",
    "    ml = y\n",
    "    for i in tqdm(p):\n",
    "        ans = strip_tags(str(df.loc[i,'long_answer']))\n",
    "        ans = ans.replace(\".\",\"[SEP]\")\n",
    "        ans = ans.replace('\"\"', '')\n",
    "        ans = ans.replace('``','')\n",
    "        ans = ans.replace(\"''\", \"\")\n",
    "        ans = \" \".join(ans.split())\n",
    "\n",
    "        qn = strip_tags(str(df.loc[i,'question']))\n",
    "        qn = qn.replace(\".\",\"[SEP]\")\n",
    "        qn = \"[CLS] \" + qn + \"[CLS]\"\n",
    "        cn = str(df.loc[i,'candidate'])\n",
    "        fn = qn + \" \" + cn +\"[CLS]\" + ans\n",
    "        toks = tokenizer.tokenize(fn)\n",
    "        tokens = toks[:ml]\n",
    "        input_ids.append(get_ids(tokens,tokenizer,ml))\n",
    "        input_masks.append(get_masks(tokens,ml))\n",
    "        input_segments.append(get_segments(tokens,ml))\n",
    "\n",
    "\n",
    "    inputs = [tf.convert_to_tensor(input_ids,dtype=float),tf.convert_to_tensor(input_masks,dtype=float),tf.convert_to_tensor(input_segments,dtype=float)\n",
    "            ]\n",
    "\n",
    "    output = tf.one_hot(tf.convert_to_tensor(df.head(x)['target'],dtype=tf.int32),depth=2)\n",
    "    \n",
    "    return inputs,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 440.62it/s]\n"
     ]
    }
   ],
   "source": [
    "inp,out = tester(20,sa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_sa_model():\n",
    "    max_seq_length = 256  # Your choice here.\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                           name=\"input_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                        name=\"segment_ids\")\n",
    "    bert_layer1 = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True,name=\"bert\")\n",
    "    \n",
    "    pooled_output, sequence_output = bert_layer1([input_ids, input_mask, segment_ids])\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Dense(128,activation=tf.nn.relu,name='combined_layer_2',kernel_initializer=tf.random_normal_initializer)(pooled_output)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(2,activation='softmax')(x)\n",
    "    model = Model(inputs=[input_ids,input_mask,segment_ids], outputs=logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_model = mk_sa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (KerasLayer)               [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "combined_layer_2 (Dense)        (None, 128)          98432       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         combined_layer_2[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 109,580,931\n",
      "Trainable params: 109,580,930\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sa_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models were trained on Google Cloud platform, refer to the scripts \"la_mod.py\" & \"sa_mod.py\" for code which ran the training in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long answer f1 score: 0.91 \n",
    "Short answer f1 score: 0.89\n",
    "\n",
    "Long answer model is predicting well but short answer is not. Will need to rethink the modeling approach for the short answer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Submission\" notebook will build the final model and prepare the submission for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

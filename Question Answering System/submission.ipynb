{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/la-weights-final/la_final.h5\n/kaggle/input/sa-weights-final/sa_final.h5\n/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n/kaggle/input/tensorflow2-question-answering/simplified-nq-train.jsonl\n/kaggle/input/tensorflow2-question-answering/sample_submission.csv\n/kaggle/input/berthub/saved_model.pb\n/kaggle/input/berthub/assets/vocab.txt\n/kaggle/input/berthub/variables/variables.index\n/kaggle/input/berthub/variables/variables.data-00000-of-00001\n/kaggle/input/weights-test/long_answer_final.h5\n/kaggle/input/zbertjointbaseline/vocab-nq.txt\n/kaggle/input/zbertjointbaseline/model_cpkt-1.data-00000-of-00002\n/kaggle/input/zbertjointbaseline/model_cpkt-1.data-00001-of-00002\n/kaggle/input/zbertjointbaseline/model_cpkt-1.index\n/kaggle/input/zbertjointbaseline/nq-train.tfrecords\n/kaggle/input/zbertjointbaseline/bert_utils.py\n/kaggle/input/zbertjointbaseline/tokenization.py\n/kaggle/input/zbertjointbaseline/nq-test.tfrecords\n/kaggle/input/zbertjointbaseline/modeling.py\n/kaggle/input/zbertjointbaseline/bert_config.json\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom transformers import BertTokenizer\nimport sys\nsys.path.append('/kaggle/input/zbertjointbaseline/')\nimport modeling \nimport time\n\ntest_path = \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\"\nsub_path = \"/kaggle/input/tensorflow2-question-answering/sample_submission.csv\"\n#la_weights_path = \"/kaggle/input/la-weights-final/la_final.h5\"\nla_weights_path = \"/kaggle/input/weights-test/long_answer_final.h5\"\nsa_weights_path = \"/kaggle/input/sa-weights-final/sa_final.h5\"\nvocab_path = \"/kaggle/input/berthub/assets/vocab.txt\"\n\n\ntokenizer = BertTokenizer(vocab_file=vocab_path)\n\npath = test_path\n\ndef get_cosine(qns, ans):\n    h = TfidfVectorizer()\n    h = h.fit([qns+ans])\n    vec1 = h.transform([qns])\n    vec2 = h.transform([ans])\n    c = cosine_similarity(vec1,vec2)\n    return float(c[0])\n\n\ncounter = 0\ncols = ['id', 'questions', 'candidate', 'span', 'jaq_sim']\n\ndef jac(df):\n    df['jaq_sim'] =\"\"\n    for i in df.index:\n        df.loc[i,'jaq_sim'] = get_jaccard_sim(df.loc[i,'question'],df.loc[i,'candidate'])\n    df.sort_values('jaq_sim',ascending=False,inplace=True)\n    return df.head(25)\n\ndef get_jaccard_sim(str1, str2): \n    a = set(str1.split()) \n    b = set(str2.split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\nwith open(path, 'rt') as json_file:\n    counter += 1\n    #ids = []\n    #qn =[]\n    #ans = []\n    #span = []\n    #cos_sim = []\n    doc_text = []\n    qid = []\n    df = pd.DataFrame()\n    for line in tqdm(json_file):\n        counter +=1\n        #if counter == 10:\n        #    break\n        t = json.loads(line)\n        text = t['document_text']\n        doc_text.append(text)\n        idn = t['example_id']\n        qid.append(idn)\n        q = t['question_text']\n        ids = []\n        qn =[]\n        ans = []\n        span = []\n        jaq_sim = []\n        for p in t['long_answer_candidates']:\n            ans1 = p['start_token']\n            ans2 = p['end_token']\n            sp_doc = t['document_text'].split(\" \")\n            span1 = str(ans1)+\":\"+str(ans2)\n            answer = \" \".join(sp_doc[ans1:ans2])\n            jac = get_jaccard_sim(q,answer)\n            ids.append(idn)\n            qn.append(q)\n            ans.append(answer)\n            span.append(span1)\n            jaq_sim.append(jac)\n        temp_df = pd.DataFrame(list(zip(ids,qn,ans,span,jaq_sim)),columns =cols)\n        temp_df.sort_values(\"jaq_sim\",ascending=False,inplace=True)\n        temp_df = temp_df.head(25)\n        #print(temp_df)\n        df = pd.concat([df,temp_df])\n        \n        \n    #df = pd.DataFrame()\n    #df['id'] = ids\n    #df['question'] = qn\n    #df['answer'] =ans\n    #df['span'] = span\n    #df['cos_sim'] = cos_sim\n    \ndf = df.rename({\"question\":'questions','answer':'candidate'},axis=1)\nend = time.time()\n#print(end-start)\ndf.reset_index(inplace=True,drop=True)\ndoc_text = pd.DataFrame({\"id\":qid,'text':doc_text})\n\ndef get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\ndef get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\nfrom html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\ndef tester(x,df,y=256,p=False,train=True):\n    input_ids = []\n    input_masks = []\n    input_segments = []\n\n    ml = y\n    for i in df.head(x).index:\n        ans = strip_tags(str(df.loc[i,'candidate']))\n        ans = ans.replace(\".\",\"[SEP]\")\n        ans = ans.replace('\"\"', '')\n        ans = ans.replace('``','')\n        ans = ans.replace(\"''\", \"\")\n        #ans = \"[CLS] \" + ans\n        ans = \" \".join(ans.split())\n\n        qn = strip_tags(str(df.loc[i,'questions']))\n        qn = qn.replace(\".\",\"[SEP]\")\n        qn = \"[CLS] \" + qn + \"[CLS]\"\n        fn = qn + \" \" + ans\n        if p:\n            print(fn)\n        tokens = tokenizer.tokenize(fn)[:ml]\n        input_ids.append(get_ids(tokens,tokenizer,ml))\n        input_masks.append(get_masks(tokens,ml))\n        input_segments.append(get_segments(tokens,ml))\n\n    if train:\n        inputs = [tf.convert_to_tensor(input_ids,dtype=float),tf.convert_to_tensor(input_masks,dtype=float),tf.convert_to_tensor(input_segments,dtype=float)\n                ]\n\n        #output = tf.convert_to_tensor(df.head(x)['target'],dtype=tf.int32)\n        #return inputs,output\n\n        output = tf.one_hot(tf.convert_to_tensor(df.head(x)['target'],dtype=tf.int32),depth=2)\n        return inputs,output\n    else:\n        inputs = [tf.convert_to_tensor(input_ids,dtype=float),tf.convert_to_tensor(input_masks,dtype=float),tf.convert_to_tensor(input_segments,dtype=float)\n                ]\n        return inputs\n    \ninp = tester(len(df),df,train=False)\n\n\ndef mk_model(path):\n    max_seq_length = 256  # Your choice here.\n    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                           name=\"input_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                       name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                        name=\"segment_ids\")\n    bert_layer1 = hub.KerasLayer(\"/kaggle/input/berthub\",trainable=True,name=\"bert\")\n    #bert_layer1 = bert_layer\n    pooled_output, sequence_output = bert_layer1([input_ids, input_mask, segment_ids])\n    x = tf.keras.layers.Dense(128,activation=tf.nn.relu,name='combined_layer_2',kernel_initializer=tf.random_normal_initializer)(pooled_output)\n    logits = tf.keras.layers.Dense(2,activation='softmax')(x)\n    model = Model(inputs=[input_ids,input_mask,segment_ids], outputs=logits)\n    model.load_weights(path)\n    return model\n\nmodel = mk_model(la_weights_path)\n\ndef predict(model,df):\n    y_preds = model.predict([i for i in inp])\n    y_probs = [max(y) for y in y_preds]\n    y = [np.argmax(y) for y in y_preds]\n    df['predictions'] = y\n    df['probs'] = y_probs\n    return df\n\n#sa_model = mk_sa_model(sa_weights_path)","execution_count":16,"outputs":[{"output_type":"stream","text":"7it [00:00, 17.65it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = predict(model,df)\np = p.sort_values('probs',ascending=False)\np = p.loc[p['predictions']==1]\np= p.drop_duplicates(subset='id',keep='first')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\ncols = ['id',\"question\",'candidate','long_answer','target']\ndef created_sa_cands(df,x=False,y=False):\n    ndf = pd.DataFrame(columns=cols)\n    qids = []\n    qs = []\n    las =[]\n    cands = []\n\n    if x:\n        df = df.loc[x:y]\n    for i in tqdm(df.index):\n        df.loc[i,'candidate'] = strip_tags(str(df.loc[i,'candidate']))\n\n        qid = df.loc[i,'id']\n        q = df.loc[i,'questions']\n        la = df.loc[i,'candidate']\n        \n        sents = split_into_sentences(la)\n        for sent in sents:\n            qids.append(qid)\n            qs.append(q)\n            las.append(la)\n            cands.append(sent)\n            \n        nes = preprocess(la)\n        text = \"\"\n        temp = []\n        for ne in nes:\n            \n            if ne[1] in [\"NNP\",\"CD\"]:\n                text += ne[0]\n                text += \" \"\n            elif ne[1] == \"CC\" and last_e in [\"NNP\",\"CD\"]:\n                text += ne[0]\n                text += \" \"\n            else:\n                if text and text not in temp:\n                    qids.append(qid)\n                    qs.append(q)\n                    las.append(la)\n                    cands.append(text)\n                    temp.append(text)\n                    \n                    text = \"\"\n            last_e = ne[1]\n        qids.append(qid)\n        qs.append(q)\n        las.append(la)\n        cands.append(\"Yes\")\n       \n        qids.append(qid)\n        qs.append(q)\n        las.append(la)\n        cands.append(\"No\")\n        \n        \n    ndf['id'] = qids\n    ndf['question'] = qs\n    ndf['long_answer'] = las\n    ndf['candidate'] = cands\n    \n    return ndf\n\nimport re\nalphabets= \"([A-Za-z])\"\nprefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = \"[.](com|net|org|io|gov)\"\n\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n    text = re.sub(websites,\"<prd>\\\\1\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n\ndef preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sa = created_sa_cands(p)","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 8/8 [00:00<00:00, 17.67it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tester(df,y=256):\n    #del df\n    df.reset_index(drop=True,inplace=True)\n    input_ids = []\n    input_masks = []\n    input_segments = []\n    p = list(df.index)\n    ml = y\n    for i in tqdm(p):\n        ans = strip_tags(str(df.loc[i,'long_answer']))\n        ans = ans.replace(\".\",\"[SEP]\")\n        ans = ans.replace('\"\"', '')\n        ans = ans.replace('``','')\n        ans = ans.replace(\"''\", \"\")\n        #ans = \"[CLS] \" + ans\n        ans = \" \".join(ans.split())\n\n        qn = strip_tags(str(df.loc[i,'question']))\n        qn = qn.replace(\".\",\"[SEP]\")\n        qn = \"[CLS] \" + qn + \"[CLS]\"\n        cn = str(df.loc[i,'candidate'])\n        fn = qn + \" \" + cn +\"[CLS]\" + ans\n        #print(fn)\n        toks = tokenizer.tokenize(fn)\n        tokens = toks[:ml]\n        input_ids.append(get_ids(tokens,tokenizer,ml))\n        input_masks.append(get_masks(tokens,ml))\n        input_segments.append(get_segments(tokens,ml))\n\n\n    inputs = [tf.convert_to_tensor(input_ids,dtype=float),tf.convert_to_tensor(input_masks,dtype=float),tf.convert_to_tensor(input_segments,dtype=float)\n            ]\n\n    \n    return inputs\n\nsa_inps = tester(df =sa.head())\n\n","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 5/5 [00:00<00:00, 314.40it/s]","name":"stderr"},{"output_type":"stream","text":"[CLS] when was i want to sing in opera written[CLS] Wilkie Bard ( born William August Smith ) ( 19 March 1874 -- 5 May 1944 ) was a popular British vaudeville and music hall entertainer and recording artist at the beginning of the 20th century .[CLS]Wilkie Bard ( born William August Smith ) ( 19 March 1874 -- 5 May 1944 ) was a popular British vaudeville and music hall entertainer and recording artist at the beginning of the 20th century [SEP] He is best known for his songs I Want to Sing in Opera and The Night Watchman [SEP]\n[CLS] when was i want to sing in opera written[CLS] He is best known for his songs `` I Want to Sing in Opera '' and `` The Night Watchman '' .[CLS]Wilkie Bard ( born William August Smith ) ( 19 March 1874 -- 5 May 1944 ) was a popular British vaudeville and music hall entertainer and recording artist at the beginning of the 20th century [SEP] He is best known for his songs I Want to Sing in Opera and The Night Watchman [SEP]\n[CLS] when was i want to sing in opera written[CLS] Wilkie Bard [CLS]Wilkie Bard ( born William August Smith ) ( 19 March 1874 -- 5 May 1944 ) was a popular British vaudeville and music hall entertainer and recording artist at the beginning of the 20th century [SEP] He is best known for his songs I Want to Sing in Opera and The Night Watchman [SEP]\n[CLS] when was i want to sing in opera written[CLS] William August Smith [CLS]Wilkie Bard ( born William August Smith ) ( 19 March 1874 -- 5 May 1944 ) was a popular British vaudeville and music hall entertainer and recording artist at the beginning of the 20th century [SEP] He is best known for his songs I Want to Sing in Opera and The Night Watchman [SEP]\n[CLS] when was i want to sing in opera written[CLS] 19 March 1874 [CLS]Wilkie Bard ( born William August Smith ) ( 19 March 1874 -- 5 May 1944 ) was a popular British vaudeville and music hall entertainer and recording artist at the beginning of the 20th century [SEP] He is best known for his songs I Want to Sing in Opera and The Night Watchman [SEP]\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model,cands_df,inp):\n    y_preds = model.predict([i for i in inp])\n    y_probs = [max(y) for y in y_preds]\n    y = [np.argmax(pred) for pred in y_preds]\n    cands_df['predictions'] = y\n    cands_df['probs'] = y_probs\n    return cands_df\nsa_model = mk_model(sa_weights_path)\ns = predict(sa_model,sa,sa_inps)\ns = s.sort_values('probs',ascending=False)\ns = s.loc[s['predictions']==1]\ns= s.drop_duplicates(subset='id',keep='first')\ns = pd.merge(s,doc_text,left_on='id',right_on='id',how='left')\ndef get_span(row):\n    h = row['text']\n    ans = row['candidate']\n    if ans.lower() == \"no\":\n        val = \"NO\"\n    elif ans.lower() == \"yes\":\n        val = \"YES\"\n    elif ans.lower() == \"nan\":\n        val = None\n    elif ans.lower() != ans.lower():\n        val = None\n    else:\n        ans_sp = ans.split(\" \")\n        l = len(ans_sp)\n        spl = h.split(\" \")\n        loop = True\n        while loop:\n            \n            try:\n                ind_1 = spl.index(ans_sp[0]) \n                for i in range(1,l):\n                    if spl[ind_1+i] != ans_sp[i]:\n                        #spl[ = spl[ind_1+1:]\n                        for t in range(ind_1+i):\n                            spl[t] = None\n                        break\n                    else:\n                        end = ind_1 +i+1\n                    if i == l-2:\n                        loop = False\n                        val = \"{}:{}\".format(ind_1,end)\n                        break\n                \n            except:\n                val =None\n                loop = False\n                break\n                \n        \n        \n    return val \n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s['sa_span'] = \"\"\ns['sa_span'] = s.apply(get_span,axis=1)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\np = pd.merge(p,s[['id','sa_span']],left_on='id',right_on='id',how='left')\nd = pd.read_csv(sub_path)\nfor i in d.index:\n    if 'long' in d.loc[i,'example_id']:\n        h = int(d.loc[i,'example_id'].split(\"_\")[0])\n        try:\n            ans = str(p.loc[p['id']==d.loc[i,'example_id'].split(\"_\")[0]].span.values[0])\n            d.loc[i,'PredictionString'] = ans\n        except:\n            d.loc[i,'PredictionString'] = None\n    if 'short' in d.loc[i,'example_id']:\n        h = int(d.loc[i,'example_id'].split(\"_\")[0])\n        try:\n            ans = str(p.loc[p['id']==d.loc[i,'example_id'].split(\"_\")[0]].sa_span.values[0])\n            d.loc[i,'PredictionString'] = ans\n        except:\n            d.loc[i,'PredictionString'] = None\nfor i in d.index:\n    if d.loc[i,'PredictionString'] == \"nan\":\n        d.loc[i,'PredictionString'] = \"\"\n    elif d.loc[i,'PredictionString'] == \"None\":\n        d.loc[i,'PredictionString'] = \"\"\n    \nd.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
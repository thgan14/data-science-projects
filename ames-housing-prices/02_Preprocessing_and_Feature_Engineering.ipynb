{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of this section:\n",
    "\n",
    "1. Preprocessing of the data to get it ready for training the model \n",
    "2. Feature engineering \n",
    "    - See if we can combine some features\n",
    "    - Then we will create some polynomial features using the columns with the highest correlation to our target variable\n",
    "    - Try simplifying some existing features. e.g columns with a rating of 1-10, trt splitting it into 3 bands of \"Bad\", \"Average\" & \"Good\"\n",
    "3. Feature selection, using methods like Recursive Feature Elimination & Variance Inflation Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.append('/Users/ganeshsivam/Mods')\n",
    "import corr\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummy variables of categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical columns can be identified by columns with dtypes as Object. However, a couple of numeric columns actually are categorical columns, these will be manually added to the cat_cols list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dummies(df):\n",
    "    cat_cols = []\n",
    "    for i in df.columns:\n",
    "        if df.dtypes[i] == \"O\":\n",
    "            cat_cols.append(i)\n",
    "    cat_cols.append(\"MS SubClass\")\n",
    "    cat_cols.append(\"Mo Sold\")\n",
    "    for c in cat_cols:\n",
    "        dums = pd.get_dummies(df[c],prefix=c)\n",
    "        df = pd.concat([df,dums],axis=1)\n",
    "        df.drop(c,inplace=True,axis=1)\n",
    "        \n",
    "    year_cols = [c for c in df.columns if \"Year\" in c or \"year\" in c or 'Yr' in c]\n",
    "    year_cols = [c for c in year_cols if \"rage\" not in c]\n",
    "    #year_cols.remove('Garage Yr Blt')\n",
    "    for c in year_cols:\n",
    "        df[c] = 2019 - df[c]\n",
    "    return df\n",
    "df = dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the datasets once before feature engineering so later on we can test the effect of feature engineering on our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv(\"./data/train_cleaned.csv\")\n",
    "test = pd.read_csv(\"./data/test_cleaned.csv\")\n",
    "\n",
    "train = dummies(train)\n",
    "test = dummies(test)\n",
    "\n",
    "def cols_sync(df1,df2):\n",
    "    df1_cols = df1.columns\n",
    "    df2_cols = df2.columns\n",
    "    df1_add = []\n",
    "    df2_add = []\n",
    "    for i in df1_cols:\n",
    "        if i not in df2_cols:\n",
    "            df2_add.append(i)\n",
    "    for i in df2_cols:\n",
    "        if i not in df1_cols:\n",
    "            df1_add.append(i)\n",
    "    for c in df1_add:\n",
    "        df1[c] = 0\n",
    "    for c in df2_add:\n",
    "        df2[c] = 0\n",
    "        \n",
    "    df2 = df2[df1.columns]\n",
    "    return df1,df2\n",
    "\n",
    "    \n",
    "\n",
    "train,test = cols_sync(train,test)\n",
    "train.to_csv(\"./data/train_bef_FE.csv\",index=False)\n",
    "test.to_csv(\"./data/test_bef_FE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will create some combined columns for the data, like \"TotalBath\" and \"Total SF\" which sums different related columns up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_features(df):\n",
    "    df[\"TotalBath\"] = df[\"Bsmt Full Bath\"] + (0.5 * df[\"Bsmt Half Bath\"]) + df[\"Full Bath\"] + (0.5 * df[\"Half Bath\"])\n",
    "    # Total SF for house (incl. basement)\n",
    "    df[\"Total SF\"] = df[\"Gr Liv Area\"] + df[\"Total Bsmt SF\"] +df[\"1st Flr SF\"] + df[\"2nd Flr SF\"]\n",
    "    # Total SF for 1st + 2nd floors\n",
    "    #df[\"AllFlrsSF\"] = df[\"1st Flr SF\"] + df[\"2nd Flr SF\"]\n",
    "    # Total SF for porch\n",
    "    df[\"AllPorchSF\"] = df[\"Open Porch SF\"] + df[\"Enclosed Porch\"] + df[\"3Ssn Porch\"] + df[\"Screen Porch\"]\n",
    "    return df\n",
    "df = comb_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(data = df.corr()['SalePrice'].values,index=df.columns,columns=[\"corelataion_target\"])\n",
    "corr_df['corelataion_target'] = abs(corr_df['corelataion_target'])\n",
    "corr_df.sort_values(\"corelataion_target\",ascending=False,inplace=True)\n",
    "top_15 = list(corr_df.index[1:16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that adds polynomial features for the columns inputed, in this case we will use the top 15 correalated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features(df,cols):\n",
    "    for c in cols:\n",
    "        h = str(c+\"s2\")\n",
    "        i = str(c+\"s3\")\n",
    "        j = str(c+\"sq\")\n",
    "        df[h] = df[c]**2\n",
    "        df[i] = df[c]**3\n",
    "        df[j] = np.sqrt(df[c])\n",
    "    return df\n",
    "df = poly_features(df,top_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a function to sync the columns in our train and test datasets, due to dummy variables columns, all the columns might not be common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cols_sync(df1,df2):\n",
    "    df1_cols = df1.columns\n",
    "    df2_cols = df2.columns\n",
    "    df1_add = []\n",
    "    df2_add = []\n",
    "    for i in df1_cols:\n",
    "        if i not in df2_cols:\n",
    "            df2_add.append(i)\n",
    "    for i in df2_cols:\n",
    "        if i not in df1_cols:\n",
    "            df1_add.append(i)\n",
    "    for c in df1_add:\n",
    "        df1[c] = 0\n",
    "    for c in df2_add:\n",
    "        df2[c] = 0\n",
    "        \n",
    "    df2 = df2[df1.columns]\n",
    "    return df1,df2\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will save different iterartions of our feature engineering to test for the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train_cleaned.csv\")\n",
    "test = pd.read_csv(\"./data/test_cleaned.csv\")\n",
    "\n",
    "def eda_proc_no_poly(df):\n",
    "    df = dummies(df)\n",
    "    df = comb_features(df)\n",
    "    #df = poly_features(df,top_15)\n",
    "    return df\n",
    "\n",
    "train = eda_proc_no_poly(train)\n",
    "test = eda_proc_no_poly(test)\n",
    "train,test = cols_sync(train,test)\n",
    "\n",
    "train.to_csv(\"./data/train_nopoly.csv\",index=False)\n",
    "test.drop(\"SalePrice\",axis=1,inplace=True)\n",
    "test.to_csv(\"./data/test_nopoly.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train_cleaned.csv\")\n",
    "test = pd.read_csv(\"./data/test_cleaned.csv\")\n",
    "                                                                                                                                                                                                           \n",
    "\n",
    "def eda_proc_final(df):\n",
    "    df = dummies(df)\n",
    "    df = comb_features(df)\n",
    "    df = poly_features(df,top_15)\n",
    "    return df\n",
    "\n",
    "train = eda_proc_final(train)\n",
    "test = eda_proc_final(test)\n",
    "train,test = cols_sync(train,test)\n",
    "\n",
    "train.to_csv(\"./data/train_final.csv\",index=False)\n",
    "test.drop(\"SalePrice\",axis=1,inplace=True)\n",
    "test.to_csv(\"./data/test_final.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
